{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.2: Sentiment Analysis - Extra\n",
    "\n",
    "#### DSC 550\n",
    "\n",
    "Taniya Adhikari 1/10/2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\taniy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from re import sub\n",
    "import multiprocessing\n",
    "from unidecode import unidecode\n",
    "import unicodedata\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "from time import time \n",
    "from collections import defaultdict\n",
    "\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n",
    "import textblob\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from textblob import TextBlob\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Pre-processing before modeling for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(df):\n",
    "    # converting all text to lowercase\n",
    "    df[\"comments\"] = df[\"comments\"].str.lower()\n",
    "    \n",
    "    # removing punctuation using string.punctuations and join()\n",
    "    df[\"comments\"] = df[\"comments\"].apply(lambda x: \"\".join([i for i in x if i not in string.punctuation]))\n",
    "    \n",
    "    # remove stop words\n",
    "    stop = stopwords.words('english')\n",
    "\n",
    "    df['stopwords'] = df[\"comments\"].apply(lambda x: len([i for i in x.split() if i in stop]))\n",
    "    df[\"comments\"]= df[\"comments\"].apply(lambda x: \" \".join(i for i in x.split() if i not in stop))\n",
    "    df['stopwords'] = df['comments'].apply(lambda x: len([i for i in x.split() if i in stop]))\n",
    "    \n",
    "    # stemming\n",
    "    porter = PorterStemmer()\n",
    "    df[\"comments\"] = df[\"comments\"].apply(lambda x: \" \".join([porter.stem(word) for word in x.split()]))\n",
    "    return df\n",
    "\n",
    "# converting string into list of words for each comment\n",
    "def text_to_list(text):\n",
    "    text = unidecode(text)\n",
    "    pattern = r'[^A-Za-z ]'\n",
    "    regex = re.compile(pattern)\n",
    "    text = regex.sub('', text).split(' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>comments</th>\n",
       "      <th>symbols</th>\n",
       "      <th>stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8998</th>\n",
       "      <td>WeekHerald</td>\n",
       "      <td>everest group amp markel mkl headtohead survey...</td>\n",
       "      <td>RE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4201</th>\n",
       "      <td>BoardCentral</td>\n",
       "      <td>adsk adsk motley fool messag board httpstcogf1...</td>\n",
       "      <td>ADSK</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17243</th>\n",
       "      <td>StockScoops</td>\n",
       "      <td>intel corpor nasdaqintc ‚Äì may data fuel invest...</td>\n",
       "      <td>INTC</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25979</th>\n",
       "      <td>mrlogoman247</td>\n",
       "      <td>rt ebaynewsroom 10 minut well kick ebay q2 201...</td>\n",
       "      <td>EBAY</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>luzgarciacalde1</td>\n",
       "      <td>rt myrollingstock gww umpq tmk earn httpstco2h...</td>\n",
       "      <td>TMK</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20914</th>\n",
       "      <td>42Stocks</td>\n",
       "      <td>httpstco0hzjumh14a dal delta air line notabl m...</td>\n",
       "      <td>DAL</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4191</th>\n",
       "      <td>MITickWatcher</td>\n",
       "      <td>top stock ta score trend sp500 pwr hfc up bmi ...</td>\n",
       "      <td>KIM</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11742</th>\n",
       "      <td>dakotafinancial</td>\n",
       "      <td>zack brokerag expect express script hold co es...</td>\n",
       "      <td>ESRX</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18935</th>\n",
       "      <td>DeweyRange</td>\n",
       "      <td>faster coin list bot ü§ë join us ‚û°Ô∏è httpstcoeyio...</td>\n",
       "      <td>RAD</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22000</th>\n",
       "      <td>INVESTonero</td>\n",
       "      <td>tsn 65c 817 stop 64</td>\n",
       "      <td>TSN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16708</th>\n",
       "      <td>OlympiaReport</td>\n",
       "      <td>investor buy anadarko petroleum apc weak https...</td>\n",
       "      <td>APC</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5357</th>\n",
       "      <td>DoThanhHuyen1</td>\n",
       "      <td>rt cryptosav mida protocol 3 pillar 1 mida fou...</td>\n",
       "      <td>MAS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19804</th>\n",
       "      <td>MarketCurrents</td>\n",
       "      <td>clorox watch goldman sach warn httpstcoqvkj5ps...</td>\n",
       "      <td>CLX</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7434</th>\n",
       "      <td>WeekHerald</td>\n",
       "      <td>barn amp nobl educ bned versu hibbett sport hi...</td>\n",
       "      <td>HIBB</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4594</th>\n",
       "      <td>OlympiaReport</td>\n",
       "      <td>trader buy dish network dish weak httpstcof54v...</td>\n",
       "      <td>DISH</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4884</th>\n",
       "      <td>OpenOutcrier</td>\n",
       "      <td>downgrad 713 avgo fun hl inci ingr ivz kgc lst...</td>\n",
       "      <td>INCY</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4536</th>\n",
       "      <td>stockbard</td>\n",
       "      <td>bwa im emo yelp mr flo two cub ago fli ms one ...</td>\n",
       "      <td>BWA</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2173</th>\n",
       "      <td>WeekHerald</td>\n",
       "      <td>o‚Äôreilli automot orli downgrad zack invest res...</td>\n",
       "      <td>ORLY</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20430</th>\n",
       "      <td>HODL_Report</td>\n",
       "      <td>nouriel patent approv fraction reserv get star...</td>\n",
       "      <td>MA</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6722</th>\n",
       "      <td>ledgerzette</td>\n",
       "      <td>discoveri commun inc common stock disca see la...</td>\n",
       "      <td>DISCA</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                source                                           comments  \\\n",
       "8998        WeekHerald  everest group amp markel mkl headtohead survey...   \n",
       "4201      BoardCentral  adsk adsk motley fool messag board httpstcogf1...   \n",
       "17243      StockScoops  intel corpor nasdaqintc ‚Äì may data fuel invest...   \n",
       "25979     mrlogoman247  rt ebaynewsroom 10 minut well kick ebay q2 201...   \n",
       "478    luzgarciacalde1  rt myrollingstock gww umpq tmk earn httpstco2h...   \n",
       "20914         42Stocks  httpstco0hzjumh14a dal delta air line notabl m...   \n",
       "4191     MITickWatcher  top stock ta score trend sp500 pwr hfc up bmi ...   \n",
       "11742  dakotafinancial  zack brokerag expect express script hold co es...   \n",
       "18935       DeweyRange  faster coin list bot ü§ë join us ‚û°Ô∏è httpstcoeyio...   \n",
       "22000      INVESTonero                                tsn 65c 817 stop 64   \n",
       "16708    OlympiaReport  investor buy anadarko petroleum apc weak https...   \n",
       "5357     DoThanhHuyen1  rt cryptosav mida protocol 3 pillar 1 mida fou...   \n",
       "19804   MarketCurrents  clorox watch goldman sach warn httpstcoqvkj5ps...   \n",
       "7434        WeekHerald  barn amp nobl educ bned versu hibbett sport hi...   \n",
       "4594     OlympiaReport  trader buy dish network dish weak httpstcof54v...   \n",
       "4884      OpenOutcrier  downgrad 713 avgo fun hl inci ingr ivz kgc lst...   \n",
       "4536         stockbard  bwa im emo yelp mr flo two cub ago fli ms one ...   \n",
       "2173        WeekHerald  o‚Äôreilli automot orli downgrad zack invest res...   \n",
       "20430      HODL_Report  nouriel patent approv fraction reserv get star...   \n",
       "6722       ledgerzette  discoveri commun inc common stock disca see la...   \n",
       "\n",
       "      symbols  stopwords  \n",
       "8998       RE          0  \n",
       "4201     ADSK          0  \n",
       "17243    INTC          0  \n",
       "25979    EBAY          0  \n",
       "478       TMK          0  \n",
       "20914     DAL          0  \n",
       "4191      KIM          0  \n",
       "11742    ESRX          0  \n",
       "18935     RAD          0  \n",
       "22000     TSN          0  \n",
       "16708     APC          0  \n",
       "5357      MAS          0  \n",
       "19804     CLX          0  \n",
       "7434     HIBB          0  \n",
       "4594     DISH          0  \n",
       "4884     INCY          0  \n",
       "4536      BWA          0  \n",
       "2173     ORLY          0  \n",
       "20430      MA          0  \n",
       "6722    DISCA          0  "
      ]
     },
     "execution_count": 627,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('stockerbot-export.csv')\n",
    "df = df.dropna().drop_duplicates().reset_index(drop=True).rename(columns={'text':'comments'})\n",
    "df1 = df.sample(n = 20)\n",
    "df2 = df1.copy()\n",
    "df2 = pre_processing(df2)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source       20\n",
       "comments     20\n",
       "symbols      20\n",
       "stopwords    20\n",
       "dtype: int64"
      ]
     },
     "execution_count": 628,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df = df2.copy()\n",
    "clean_df.comments = clean_df.comments.apply(lambda x: text_to_list(x))\n",
    "clean_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 13:40:59: collecting all words and their counts\n",
      "INFO - 13:40:59: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 13:40:59: PROGRESS: at sentence #7, processed 86 words and 154 word types\n",
      "INFO - 13:40:59: PROGRESS: at sentence #14, processed 165 words and 292 word types\n",
      "INFO - 13:40:59: collected 417 word types from a corpus of 235 words (unigram + bigrams) and 20 sentences\n",
      "INFO - 13:40:59: using 417 counts as vocab in Phrases<0 vocab, min_count=1, threshold=10.0, max_vocab_size=40000000>\n",
      "INFO - 13:40:59: source_vocab length 417\n",
      "INFO - 13:40:59: Phraser built with 0 phrasegrams\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['discoveri',\n",
       " 'commun',\n",
       " 'inc',\n",
       " 'common',\n",
       " 'stock',\n",
       " 'disca',\n",
       " 'see',\n",
       " 'larg',\n",
       " 'growth',\n",
       " 'short',\n",
       " 'interest',\n",
       " 'httpstcomdyybie']"
      ]
     },
     "execution_count": 629,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extraction of phrases and bigram model\n",
    "sent = [row for row in clean_df.comments]\n",
    "phrases = Phrases(sent, min_count=1, progress_per=7)\n",
    "bigram = Phraser(phrases)\n",
    "sentences = bigram[sent]\n",
    "sentences[19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - 13:43:28: consider setting layer size to a multiple of 4 for greater performance\n",
      "INFO - 13:43:28: collecting all words and their counts\n",
      "INFO - 13:43:28: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 13:43:28: collected 202 word types from a corpus of 220 raw words and 20 sentences\n",
      "INFO - 13:43:28: Loading a fresh vocabulary\n",
      "INFO - 13:43:28: effective_min_count=1 retains 202 unique words (100% of original 202, drops 0)\n",
      "INFO - 13:43:28: effective_min_count=1 leaves 220 word corpus (100% of original 220, drops 0)\n",
      "INFO - 13:43:28: deleting the raw counts dictionary of 202 items\n",
      "INFO - 13:43:28: sample=0.001 downsamples 202 most-common words\n",
      "INFO - 13:43:28: downsampling leaves estimated 142 word corpus (64.8% of prior 220)\n",
      "INFO - 13:43:28: estimated required memory for 202 words and 2 dimensions: 104232 bytes\n",
      "INFO - 13:43:28: resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "w2v_model = Word2Vec(min_count=1, size=2)\n",
    "\n",
    "# building vocab\n",
    "w2v_model.build_vocab(sentences, progress_per=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 13:43:29: training model with 3 workers on 202 vocabulary and 2 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "INFO - 13:43:29: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 13:43:29: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 13:43:29: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 13:43:29: EPOCH - 1 : training on 220 raw words (146 effective words) took 0.0s, 49930 effective words/s\n",
      "INFO - 13:43:29: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 13:43:29: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 13:43:29: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 13:43:29: EPOCH - 2 : training on 220 raw words (146 effective words) took 0.0s, 48047 effective words/s\n",
      "INFO - 13:43:29: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 13:43:29: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 13:43:29: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 13:43:29: EPOCH - 3 : training on 220 raw words (156 effective words) took 0.0s, 48529 effective words/s\n",
      "INFO - 13:43:29: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 13:43:29: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 13:43:29: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 13:43:29: EPOCH - 4 : training on 220 raw words (150 effective words) took 0.0s, 36380 effective words/s\n",
      "INFO - 13:43:29: training on a 880 raw words (598 effective words) took 0.0s, 19646 effective words/s\n",
      "WARNING - 13:43:29: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "INFO - 13:43:29: precomputing L2-norms of word weight vectors\n"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=4, report_delay=1)\n",
    "w2v_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=202, size=2, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "print(w2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 13:43:31: saving Word2Vec object under word2vec.model, separately None\n",
      "INFO - 13:43:31: not storing attribute vectors_norm\n",
      "INFO - 13:43:31: not storing attribute cum_table\n",
      "INFO - 13:43:31: saved word2vec.model\n"
     ]
    }
   ],
   "source": [
    "w2v_model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 13:43:32: loading Word2Vec object from word2vec.model\n",
      "INFO - 13:43:32: loading wv recursively from word2vec.model.wv.* with mmap=None\n",
      "INFO - 13:43:32: setting ignored attribute vectors_norm to None\n",
      "INFO - 13:43:32: loading vocabulary recursively from word2vec.model.vocabulary.* with mmap=None\n",
      "INFO - 13:43:32: loading trainables recursively from word2vec.model.trainables.* with mmap=None\n",
      "INFO - 13:43:32: setting ignored attribute cum_table to None\n",
      "INFO - 13:43:32: loaded word2vec.model\n"
     ]
    }
   ],
   "source": [
    "word_vectors = Word2Vec.load(\"word2vec.model\").wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-means clustering to create sentiment dictionary for each word\n",
    "model = KMeans(n_clusters=2, max_iter=1000).fit(X=word_vectors.vectors.astype(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = model.cluster_centers_[0]\n",
    "negative = model.cluster_centers_[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('masternod', 0.9999260902404785),\n",
       " ('co', 0.9998615384101868),\n",
       " ('market', 0.9998602867126465),\n",
       " ('gww', 0.9994181394577026),\n",
       " ('ebay', 0.9992091655731201),\n",
       " ('headtohead', 0.9971724152565002),\n",
       " ('burn', 0.9960905909538269),\n",
       " ('earn', 0.9917668700218201),\n",
       " ('httpstconweopozb', 0.9907124638557434),\n",
       " ('apc', 0.9905270934104919)]"
      ]
     },
     "execution_count": 656,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similar_by_vector(model.cluster_centers_[0], restrict_vocab=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = pd.DataFrame(word_vectors.vocab.keys())\n",
    "words.columns = ['words']\n",
    "words['vectors'] = words.words.apply(lambda x: word_vectors[f'{x}'])\n",
    "words['cluster'] = words.vectors.apply(lambda x: model.predict([np.array(x)]))\n",
    "words.cluster = words.cluster.apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>vectors</th>\n",
       "      <th>cluster</th>\n",
       "      <th>cluster_value</th>\n",
       "      <th>closeness_score</th>\n",
       "      <th>sentiment_coeff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>everest</td>\n",
       "      <td>[0.4738456, -0.88060796]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.386529</td>\n",
       "      <td>1.386529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>group</td>\n",
       "      <td>[-0.8107721, 0.58536184]</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.997127</td>\n",
       "      <td>-0.997127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>amp</td>\n",
       "      <td>[0.66654176, -0.7454677]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.146746</td>\n",
       "      <td>1.146746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>markel</td>\n",
       "      <td>[-0.9536343, -0.30096793]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.220324</td>\n",
       "      <td>1.220324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mkl</td>\n",
       "      <td>[0.99999994, 0.0004636285]</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.969323</td>\n",
       "      <td>-0.969323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>larg</td>\n",
       "      <td>[0.22235467, -0.9749659]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.805448</td>\n",
       "      <td>1.805448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>growth</td>\n",
       "      <td>[-0.6832685, 0.73016727]</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.138297</td>\n",
       "      <td>-1.138297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>short</td>\n",
       "      <td>[-0.35384795, 0.935303]</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.614675</td>\n",
       "      <td>-1.614675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>interest</td>\n",
       "      <td>[-0.59522563, 0.80355865]</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.245874</td>\n",
       "      <td>-1.245874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>httpstcomdyybie</td>\n",
       "      <td>[-0.6219093, 0.7830893]</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.212121</td>\n",
       "      <td>-1.212121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>202 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               words                     vectors  cluster  cluster_value  \\\n",
       "0            everest    [0.4738456, -0.88060796]        1              1   \n",
       "1              group    [-0.8107721, 0.58536184]        0             -1   \n",
       "2                amp    [0.66654176, -0.7454677]        1              1   \n",
       "3             markel   [-0.9536343, -0.30096793]        1              1   \n",
       "4                mkl  [0.99999994, 0.0004636285]        0             -1   \n",
       "..               ...                         ...      ...            ...   \n",
       "197             larg    [0.22235467, -0.9749659]        1              1   \n",
       "198           growth    [-0.6832685, 0.73016727]        0             -1   \n",
       "199            short     [-0.35384795, 0.935303]        0             -1   \n",
       "200         interest   [-0.59522563, 0.80355865]        0             -1   \n",
       "201  httpstcomdyybie     [-0.6219093, 0.7830893]        0             -1   \n",
       "\n",
       "     closeness_score  sentiment_coeff  \n",
       "0           1.386529         1.386529  \n",
       "1           0.997127        -0.997127  \n",
       "2           1.146746         1.146746  \n",
       "3           1.220324         1.220324  \n",
       "4           0.969323        -0.969323  \n",
       "..               ...              ...  \n",
       "197         1.805448         1.805448  \n",
       "198         1.138297        -1.138297  \n",
       "199         1.614675        -1.614675  \n",
       "200         1.245874        -1.245874  \n",
       "201         1.212121        -1.212121  \n",
       "\n",
       "[202 rows x 6 columns]"
      ]
     },
     "execution_count": 659,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words['cluster_value'] = [1 if i==1 else -1 for i in words.cluster]\n",
    "words['closeness_score'] = words.apply(lambda x: 1/(model.transform([x.vectors]).min()), axis=1)\n",
    "words['sentiment_coeff'] = words.closeness_score * words.cluster_value\n",
    "\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analyzer(wordlist):\n",
    "    cluster_count = {}\n",
    "    for w in wordlist:\n",
    "        x = cluster_freq(w, words)\n",
    "        if x in cluster_count:\n",
    "            cluster_count[x] = cluster_count[x] + 1\n",
    "        else:\n",
    "            cluster_count[x] = 1\n",
    "    sentiment = \"\"              \n",
    "    if '-1' in cluster_count.keys() and '1' in cluster_count.keys():\n",
    "        pos = cluster_count.get('1')\n",
    "        neg = cluster_count.get('-1')\n",
    "        if pos >= neg:\n",
    "            sentiment = \"Positive\"\n",
    "        else:\n",
    "            sentiment = \"Negative\"\n",
    "\n",
    "    elif '-1' not in cluster_count.keys():\n",
    "        sentiment = \"Positive\"\n",
    "    elif '1' not in cluster_count.keys():\n",
    "        sentiment = \"Negative\"\n",
    "    else:\n",
    "        None\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_freq(word, df):\n",
    "    x = ''\n",
    "    for i, r in df.iterrows():\n",
    "        if r['words'] == word:\n",
    "            x = str(r['cluster_value'])\n",
    "        else:\n",
    "            None\n",
    "    return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cluster_count = {}\n",
    "sentiment = []\n",
    "result = \"\"\n",
    "for i, r in df2.iterrows():\n",
    "    comment = r['comments']\n",
    "    wordlist = comment.split()\n",
    "    result = sentiment_analyzer(wordlist)\n",
    "    sentiment.append(result)\n",
    "    \n",
    "df1['sentiment'] = sentiment    \n",
    "df2['sentiment'] = sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_func(comments):\n",
    "    pol = TextBlob(comments).sentiment.polarity\n",
    "    if pol >= 0:\n",
    "        x = \"Positive\"\n",
    "    else:\n",
    "        x = \"Negative\"\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>comments</th>\n",
       "      <th>symbols</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>textblob_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8998</th>\n",
       "      <td>WeekHerald</td>\n",
       "      <td>Everest Re Group $RE &amp;amp; Markel $MKL Head-To...</td>\n",
       "      <td>RE</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4201</th>\n",
       "      <td>BoardCentral</td>\n",
       "      <td>$ADSK \"ADSK\" on The Motley Fool message boards...</td>\n",
       "      <td>ADSK</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17243</th>\n",
       "      <td>StockScoops</td>\n",
       "      <td>Intel Corporation (NASDAQ:INTC) ‚Äì May This Dat...</td>\n",
       "      <td>INTC</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25979</th>\n",
       "      <td>mrlogoman247</td>\n",
       "      <td>RT @eBayNewsroom: In 10 minutes we'll kick off...</td>\n",
       "      <td>EBAY</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>luzgarciacalde1</td>\n",
       "      <td>RT @myrollingstocks: $GWW $UMPQ $TMK before ea...</td>\n",
       "      <td>TMK</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20914</th>\n",
       "      <td>42Stocks</td>\n",
       "      <td>https://t.co/0HzjUMh14A $DAL Delta Air Lines n...</td>\n",
       "      <td>DAL</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4191</th>\n",
       "      <td>MITickWatcher</td>\n",
       "      <td>Top stocks with TA score trending DOWN (SP500)...</td>\n",
       "      <td>KIM</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11742</th>\n",
       "      <td>dakotafinancial</td>\n",
       "      <td>Zacks: Brokerages Expect Express Scripts Holdi...</td>\n",
       "      <td>ESRX</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18935</th>\n",
       "      <td>DeweyRange</td>\n",
       "      <td>We are faster than any other coin listing bots...</td>\n",
       "      <td>RAD</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22000</th>\n",
       "      <td>INVESTonero</td>\n",
       "      <td>$TSN 65C 8/17 Stop @ 64</td>\n",
       "      <td>TSN</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16708</th>\n",
       "      <td>OlympiaReport</td>\n",
       "      <td>Investors Buy Anadarko Petroleum $APC on Weakn...</td>\n",
       "      <td>APC</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5357</th>\n",
       "      <td>DoThanhHuyen1</td>\n",
       "      <td>RT @CryptoSave: Midas Protocol 3 Pillars: 1) M...</td>\n",
       "      <td>MAS</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19804</th>\n",
       "      <td>MarketCurrents</td>\n",
       "      <td>Clorox on watch after Goldman Sachs warning ht...</td>\n",
       "      <td>CLX</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7434</th>\n",
       "      <td>WeekHerald</td>\n",
       "      <td>Barnes &amp;amp; Noble Education $BNED versus Hibb...</td>\n",
       "      <td>HIBB</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4594</th>\n",
       "      <td>OlympiaReport</td>\n",
       "      <td>Traders Buy DISH Network $DISH on Weakness htt...</td>\n",
       "      <td>DISH</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4884</th>\n",
       "      <td>OpenOutcrier</td>\n",
       "      <td>Downgrades 7/13: $AVGO $FUN $HL $INCY $INGR $I...</td>\n",
       "      <td>INCY</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4536</th>\n",
       "      <td>stockbard</td>\n",
       "      <td>$BWA! $IM $TOO $EMO $YELP $MR $FLO $TWO $CUBS ...</td>\n",
       "      <td>BWA</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2173</th>\n",
       "      <td>WeekHerald</td>\n",
       "      <td>O‚ÄôReilly Automotive $ORLY Downgraded by Zacks ...</td>\n",
       "      <td>ORLY</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20430</th>\n",
       "      <td>HODL_Report</td>\n",
       "      <td>@Nouriel $MA patent approval!  Fractional rese...</td>\n",
       "      <td>MA</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6722</th>\n",
       "      <td>ledgerzette</td>\n",
       "      <td>DISCOVERY COMMUNICATIONS INC. Common Stock $DI...</td>\n",
       "      <td>DISCA</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                source                                           comments  \\\n",
       "8998        WeekHerald  Everest Re Group $RE &amp; Markel $MKL Head-To...   \n",
       "4201      BoardCentral  $ADSK \"ADSK\" on The Motley Fool message boards...   \n",
       "17243      StockScoops  Intel Corporation (NASDAQ:INTC) ‚Äì May This Dat...   \n",
       "25979     mrlogoman247  RT @eBayNewsroom: In 10 minutes we'll kick off...   \n",
       "478    luzgarciacalde1  RT @myrollingstocks: $GWW $UMPQ $TMK before ea...   \n",
       "20914         42Stocks  https://t.co/0HzjUMh14A $DAL Delta Air Lines n...   \n",
       "4191     MITickWatcher  Top stocks with TA score trending DOWN (SP500)...   \n",
       "11742  dakotafinancial  Zacks: Brokerages Expect Express Scripts Holdi...   \n",
       "18935       DeweyRange  We are faster than any other coin listing bots...   \n",
       "22000      INVESTonero                            $TSN 65C 8/17 Stop @ 64   \n",
       "16708    OlympiaReport  Investors Buy Anadarko Petroleum $APC on Weakn...   \n",
       "5357     DoThanhHuyen1  RT @CryptoSave: Midas Protocol 3 Pillars: 1) M...   \n",
       "19804   MarketCurrents  Clorox on watch after Goldman Sachs warning ht...   \n",
       "7434        WeekHerald  Barnes &amp; Noble Education $BNED versus Hibb...   \n",
       "4594     OlympiaReport  Traders Buy DISH Network $DISH on Weakness htt...   \n",
       "4884      OpenOutcrier  Downgrades 7/13: $AVGO $FUN $HL $INCY $INGR $I...   \n",
       "4536         stockbard  $BWA! $IM $TOO $EMO $YELP $MR $FLO $TWO $CUBS ...   \n",
       "2173        WeekHerald  O‚ÄôReilly Automotive $ORLY Downgraded by Zacks ...   \n",
       "20430      HODL_Report  @Nouriel $MA patent approval!  Fractional rese...   \n",
       "6722       ledgerzette  DISCOVERY COMMUNICATIONS INC. Common Stock $DI...   \n",
       "\n",
       "      symbols sentiment textblob_sentiment  \n",
       "8998       RE  Positive           Positive  \n",
       "4201     ADSK  Positive           Positive  \n",
       "17243    INTC  Negative           Positive  \n",
       "25979    EBAY  Negative           Positive  \n",
       "478       TMK  Negative           Positive  \n",
       "20914     DAL  Positive           Positive  \n",
       "4191      KIM  Negative           Positive  \n",
       "11742    ESRX  Negative           Positive  \n",
       "18935     RAD  Positive           Positive  \n",
       "22000     TSN  Positive           Positive  \n",
       "16708     APC  Negative           Negative  \n",
       "5357      MAS  Positive           Positive  \n",
       "19804     CLX  Positive           Positive  \n",
       "7434     HIBB  Negative           Positive  \n",
       "4594     DISH  Positive           Negative  \n",
       "4884     INCY  Positive           Positive  \n",
       "4536      BWA  Positive           Positive  \n",
       "2173     ORLY  Negative           Positive  \n",
       "20430      MA  Negative           Positive  \n",
       "6722    DISCA  Negative           Negative  "
      ]
     },
     "execution_count": 664,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['textblob_sentiment']= df2['comments'].apply(sentiment_func)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
